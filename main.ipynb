{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e7ce39a-4366-486f-a396-904fa466c9f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 163\u001b[0m\n\u001b[1;32m    161\u001b[0m         ytrue \u001b[38;5;241m=\u001b[39m test[target]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m-\u001b[39mh)\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m-\u001b[39mh]\n\u001b[1;32m    162\u001b[0m         ypred \u001b[38;5;241m=\u001b[39m test[target]\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m-\u001b[39mh]\n\u001b[0;32m--> 163\u001b[0m         baseline\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m: h, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaive\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mrmse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mytrue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mypred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m})\n\u001b[1;32m    164\u001b[0m baseline_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(baseline)\n\u001b[1;32m    165\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(OUTPUT_DIR, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 59\u001b[0m, in \u001b[0;36mrmse\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrmse\u001b[39m(y_true, y_pred):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/sklearn/metrics/_regression.py:474\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    405\u001b[0m     {\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    416\u001b[0m ):\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    0.825...\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    478\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/sklearn/metrics/_regression.py:100\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m--> 100\u001b[0m y_true \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.10/site-packages/sklearn/utils/validation.py:969\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m--> 969\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    970\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    971\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    972\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m    973\u001b[0m         )\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    976\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "import os, math, gc, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, RandomForestClassifier,\n",
    "    IsolationForest, StackingRegressor, StackingClassifier,\n",
    "    HistGradientBoostingRegressor, HistGradientBoostingClassifier\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(0)\n",
    "\n",
    "# Optional libs\n",
    "HAS_XGB = HAS_LGBM = HAS_CAT = HAS_NGB = False\n",
    "try:\n",
    "    from xgboost import XGBRegressor, XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "    HAS_LGBM = True\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "    HAS_CAT = True\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    from ngboost import NGBRegressor\n",
    "    from ngboost.distns import Normal\n",
    "    HAS_NGB = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "DATA_PATH = \"data/AirQualityUCI.csv\"\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "FIG_DIR = os.path.join(OUTPUT_DIR, \"figures\")\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "TARGETS = [\"CO(GT)\", \"NMHC(GT)\", \"C6H6(GT)\", \"NOx(GT)\", \"NO2(GT)\"]\n",
    "METEO = [\"T\", \"RH\", \"AH\"]\n",
    "ALL_FEATURES_RAW = TARGETS + METEO\n",
    "HORIZONS = [1, 6, 12, 24]\n",
    "LAGS = [1, 6, 12, 24]\n",
    "MAS = [3, 6, 12, 24]\n",
    "TRAIN_END = \"2004-12-31 23:00\"\n",
    "TEST_END = \"2005-02-28 23:00\"\n",
    "USE_CURRENT_LEVELS = True\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def load_and_clean(path):\n",
    "    df = pd.read_csv(path, sep=';', decimal=',', na_values=['NA', 'NaN', ''])\n",
    "    if df.columns[-1] == \"\":\n",
    "        df = df.iloc[:, :-1]\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Date\"].astype(str) + \" \" + df[\"Time\"].astype(str),\n",
    "                                     dayfirst=True, errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Timestamp\"]).sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "    num_cols = [c for c in df.columns if c not in [\"Date\",\"Time\",\"Timestamp\"]]\n",
    "    for c in num_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        df.loc[df[c] == -200, c] = np.nan\n",
    "    return df\n",
    "\n",
    "def impute_base(df, cols):\n",
    "    df = df.copy()\n",
    "    df[cols] = df[cols].ffill()\n",
    "    for c in cols:\n",
    "        weekday_med = df.groupby(df[\"Timestamp\"].dt.weekday)[c].transform(\"median\")\n",
    "        df[c] = df[c].fillna(weekday_med)\n",
    "    df[cols] = df[cols].fillna(df[cols].median())\n",
    "    return df\n",
    "\n",
    "def add_time_features(df):\n",
    "    df = df.copy()\n",
    "    ts = df[\"Timestamp\"]\n",
    "    df[\"hour\"] = ts.dt.hour\n",
    "    df[\"weekday\"] = ts.dt.weekday\n",
    "    df[\"month\"] = ts.dt.month\n",
    "    df[\"is_weekend\"] = (df[\"weekday\"] >= 5).astype(int)\n",
    "    return df\n",
    "\n",
    "def add_lag_ma_features(df, base_cols, lags, mas):\n",
    "    df = df.copy()\n",
    "    for c in base_cols:\n",
    "        for L in lags:\n",
    "            df[f\"{c}_lag{L}\"] = df[c].shift(L)\n",
    "        for w in mas:\n",
    "            df[f\"{c}_ma{w}\"] = df[c].rolling(w, min_periods=max(1, w//2)).mean()\n",
    "    return df\n",
    "\n",
    "def temporal_split(df, train_end, test_end):\n",
    "    train = df[df[\"Timestamp\"] <= train_end].copy()\n",
    "    test = df[(df[\"Timestamp\"] > train_end) & (df[\"Timestamp\"] <= test_end)].copy()\n",
    "    return train, test\n",
    "\n",
    "def build_feature_list(df):\n",
    "    feats = []\n",
    "    for c in ALL_FEATURES_RAW:\n",
    "        if USE_CURRENT_LEVELS:\n",
    "            feats.append(c)\n",
    "        for L in LAGS:\n",
    "            feats.append(f\"{c}_lag{L}\")\n",
    "        for w in MAS:\n",
    "            feats.append(f\"{c}_ma{w}\")\n",
    "    feats += [\"hour\", \"weekday\", \"month\", \"is_weekend\"]\n",
    "    return [c for c in feats if c in df.columns]\n",
    "\n",
    "def make_supervised(train, test, target, horizon, feature_cols):\n",
    "    ytr = train[target].shift(-horizon)\n",
    "    yte = test[target].shift(-horizon)\n",
    "    Xtr = train[feature_cols].iloc[:-horizon].copy()\n",
    "    ytr = ytr.iloc[:-horizon].copy()\n",
    "    Xte = test[feature_cols].iloc[:-horizon].copy()\n",
    "    yte = yte.iloc[:-horizon].copy()\n",
    "    scaler = StandardScaler()\n",
    "    Xtr = pd.DataFrame(scaler.fit_transform(Xtr), columns=Xtr.columns, index=Xtr.index)\n",
    "    Xte = pd.DataFrame(scaler.transform(Xte), columns=Xte.columns, index=Xte.index)\n",
    "    return Xtr, ytr.values.ravel(), Xte, yte.values.ravel()\n",
    "\n",
    "def discretize_CO(values):\n",
    "    bins = [-np.inf, 1.5, 2.5, np.inf]\n",
    "    labels = [0, 1, 2]\n",
    "    return pd.cut(values, bins=bins, labels=labels).astype(int)\n",
    "\n",
    "def plot_series(y_true, y_pred, title, path):\n",
    "    plt.figure(figsize=(10,3.2))\n",
    "    n = min(500, len(y_true))\n",
    "    plt.plot(range(n), y_true[:n], label=\"True\")\n",
    "    plt.plot(range(n), y_pred[:n], label=\"Pred\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "# Load + features\n",
    "df = load_and_clean(DATA_PATH)\n",
    "df = impute_base(df, ALL_FEATURES_RAW)\n",
    "df = add_time_features(df)\n",
    "df = add_lag_ma_features(df, ALL_FEATURES_RAW, LAGS, MAS)\n",
    "feature_cols = build_feature_list(df)\n",
    "train, test = temporal_split(df, TRAIN_END, TEST_END)\n",
    "\n",
    "# Naive baselines\n",
    "baseline = []\n",
    "for target in TARGETS:\n",
    "    for h in HORIZONS:\n",
    "        ytrue = test[target].shift(-h).iloc[:-h]\n",
    "        ypred = test[target].iloc[:-h]\n",
    "        baseline.append({\"target\": target, \"h\": h, \"model\": \"Naive\", \"rmse\": rmse(ytrue.values, ypred.values)})\n",
    "baseline_df = pd.DataFrame(baseline)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "baseline_df.to_csv(os.path.join(OUTPUT_DIR, \"results_regression_baseline.csv\"), index=False)\n",
    "\n",
    "# Regression models + ensembling\n",
    "reg_rows = []\n",
    "for target in TARGETS:\n",
    "    for h in HORIZONS:\n",
    "        Xtr, ytr, Xte, yte = make_supervised(train, test, target, h, feature_cols)\n",
    "        models = {\n",
    "            \"Linear\": LinearRegression(),\n",
    "            \"HGBR\": HistGradientBoostingRegressor(max_iter=400, learning_rate=0.06, random_state=0),\n",
    "            \"RF\": RandomForestRegressor(n_estimators=500, n_jobs=-1, random_state=0),\n",
    "        }\n",
    "        if HAS_XGB:\n",
    "            models[\"XGB\"] = XGBRegressor(\n",
    "                n_estimators=600, max_depth=8, learning_rate=0.05,\n",
    "                subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "                random_state=0, tree_method=\"hist\", n_jobs=-1\n",
    "            )\n",
    "        if HAS_LGBM:\n",
    "            models[\"LGBM\"] = LGBMRegressor(\n",
    "                n_estimators=800, num_leaves=63, learning_rate=0.05,\n",
    "                subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "                random_state=0\n",
    "            )\n",
    "        if HAS_CAT:\n",
    "            models[\"CatBoost\"] = CatBoostRegressor(\n",
    "                iterations=800, depth=8, learning_rate=0.05,\n",
    "                loss_function=\"RMSE\", random_seed=0, verbose=False\n",
    "            )\n",
    "        if HAS_NGB:\n",
    "            models[\"NGBoost\"] = NGBRegressor(Dist=Normal, n_estimators=600, learning_rate=0.05,\n",
    "                                             verbose=False, random_state=0)\n",
    "\n",
    "        preds = {}\n",
    "        for name, mdl in models.items():\n",
    "            mdl.fit(Xtr, ytr)\n",
    "            p = mdl.predict(Xte)\n",
    "            preds[name] = p\n",
    "            reg_rows.append({\"target\": target, \"h\": h, \"model\": name, \"rmse\": rmse(yte, p)})\n",
    "\n",
    "        blend = np.mean(np.column_stack(list(preds.values())), axis=1)\n",
    "        reg_rows.append({\"target\": target, \"h\": h, \"model\": \"BlendMean\", \"rmse\": rmse(yte, blend)})\n",
    "\n",
    "        base_estimators = []\n",
    "        for nm in preds.keys():\n",
    "            if nm == \"Linear\":\n",
    "                base_estimators.append((nm, LinearRegression()))\n",
    "            elif nm == \"HGBR\":\n",
    "                base_estimators.append((nm, HistGradientBoostingRegressor(max_iter=400, learning_rate=0.06, random_state=0)))\n",
    "            elif nm == \"RF\":\n",
    "                base_estimators.append((nm, RandomForestRegressor(n_estimators=500, n_jobs=-1, random_state=0)))\n",
    "            elif nm == \"XGB\" and HAS_XGB:\n",
    "                base_estimators.append((nm, XGBRegressor(n_estimators=600, max_depth=8, learning_rate=0.05,\n",
    "                                                         subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "                                                         random_state=0, tree_method=\"hist\", n_jobs=-1)))\n",
    "            elif nm == \"LGBM\" and HAS_LGBM:\n",
    "                base_estimators.append((nm, LGBMRegressor(n_estimators=800, num_leaves=63, learning_rate=0.05,\n",
    "                                                          subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "                                                          random_state=0)))\n",
    "            elif nm == \"CatBoost\" and HAS_CAT:\n",
    "                base_estimators.append((nm, CatBoostRegressor(iterations=800, depth=8, learning_rate=0.05,\n",
    "                                                              loss_function=\"RMSE\", random_seed=0, verbose=False)))\n",
    "            elif nm == \"NGBoost\" and HAS_NGB:\n",
    "                base_estimators.append((nm, NGBRegressor(Dist=Normal, n_estimators=600, learning_rate=0.05,\n",
    "                                                         verbose=False, random_state=0)))\n",
    "        if len(base_estimators) >= 2:\n",
    "            stack = StackingRegressor(estimators=base_estimators, final_estimator=LinearRegression())\n",
    "            stack.fit(Xtr, ytr)\n",
    "            sp = stack.predict(Xte)\n",
    "            reg_rows.append({\"target\": target, \"h\": h, \"model\": \"StackingLR\", \"rmse\": rmse(yte, sp)})\n",
    "\n",
    "            best_row = min([r for r in reg_rows if r[\"target\"]==target and r[\"h\"]==h and r[\"model\"] in models],\n",
    "                           key=lambda x: x[\"rmse\"])\n",
    "            best_pred = preds[best_row[\"model\"]]\n",
    "            plot_series(yte, best_pred, f\"{target} +{h}h Best:{best_row['model']}\",\n",
    "                        os.path.join(FIG_DIR, f\"{target}_h{h}_best.png\"))\n",
    "            plot_series(yte, blend, f\"{target} +{h}h BlendMean\",\n",
    "                        os.path.join(FIG_DIR, f\"{target}_h{h}_blend.png\"))\n",
    "            plot_series(yte, sp, f\"{target} +{h}h Stacking\",\n",
    "                        os.path.join(FIG_DIR, f\"{target}_h{h}_stack.png\"))\n",
    "        else:\n",
    "            best_row = min([r for r in reg_rows if r[\"target\"]==target and r[\"h\"]==h and r[\"model\"] in models],\n",
    "                           key=lambda x: x[\"rmse\"])\n",
    "            best_pred = preds[best_row[\"model\"]]\n",
    "            plot_series(yte, best_pred, f\"{target} +{h}h Best:{best_row['model']}\",\n",
    "                        os.path.join(FIG_DIR, f\"{target}_h{h}_best.png\"))\n",
    "            plot_series(yte, blend, f\"{target} +{h}h BlendMean\",\n",
    "                        os.path.join(FIG_DIR, f\"{target}_h{h}_blend.png\"))\n",
    "        gc.collect()\n",
    "\n",
    "reg_df = pd.DataFrame(reg_rows + baseline_df.to_dict(\"records\"))\n",
    "reg_df.to_csv(os.path.join(OUTPUT_DIR, \"results_regression.csv\"), index=False)\n",
    "\n",
    "# Robust training via anomaly filtering (example CO +1h)\n",
    "Xtr, ytr, Xte, yte = make_supervised(train, test, \"CO(GT)\", 1, feature_cols)\n",
    "base = HistGradientBoostingRegressor(max_iter=400, learning_rate=0.06, random_state=0).fit(Xtr, ytr)\n",
    "resid = ytr - base.predict(Xtr)\n",
    "tau = np.quantile(np.abs(resid), 0.95)\n",
    "mask = (np.abs(resid) <= tau)\n",
    "iso = IsolationForest(n_estimators=300, contamination=0.05, random_state=0).fit_predict(np.c_[Xtr, ytr]) == 1\n",
    "robust_mask = mask & iso\n",
    "if robust_mask.mean() < 0.7:\n",
    "    robust_mask = mask\n",
    "robust = HistGradientBoostingRegressor(max_iter=400, learning_rate=0.06, random_state=0).fit(Xtr[robust_mask], ytr[robust_mask])\n",
    "rmse_base = rmse(yte, base.predict(Xte))\n",
    "rmse_rob = rmse(yte, robust.predict(Xte))\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.hist(np.clip(resid, -5, 5), bins=60)\n",
    "plt.title(\"CO +1h Train Residuals (clipped)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, \"CO_h1_residual_hist.png\"), dpi=160)\n",
    "plt.close()\n",
    "\n",
    "# CO level classification\n",
    "cls_rows = []\n",
    "for h in HORIZONS:\n",
    "    ytr_cls = discretize_CO(train[\"CO(GT)\"].shift(-h)).iloc[:-h]\n",
    "    yte_cls = discretize_CO(test[\"CO(GT)\"].shift(-h)).iloc[:-h]\n",
    "    Xtr, _, Xte, _ = make_supervised(train, test, \"CO(GT)\", h, feature_cols)\n",
    "    mask_valid = (~pd.isna(ytr_cls)).values\n",
    "    Xtr_c = Xtr.iloc[mask_valid]\n",
    "    ytr_c = ytr_cls.values[mask_valid]\n",
    "\n",
    "    clfs = {\n",
    "        \"LogReg\": LogisticRegression(max_iter=2000, multi_class=\"multinomial\", random_state=0),\n",
    "        \"HGBC\": HistGradientBoostingClassifier(max_iter=400, learning_rate=0.08, random_state=0),\n",
    "        \"RF\": RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=0),\n",
    "    }\n",
    "    if HAS_XGB:\n",
    "        clfs[\"XGBc\"] = XGBClassifier(\n",
    "            n_estimators=600, max_depth=8, learning_rate=0.06,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "            random_state=0, tree_method=\"hist\", n_jobs=-1\n",
    "        )\n",
    "    if HAS_LGBM:\n",
    "        clfs[\"LGBMc\"] = LGBMClassifier(\n",
    "            n_estimators=800, num_leaves=63, learning_rate=0.06,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "            random_state=0\n",
    "        )\n",
    "    if HAS_CAT:\n",
    "        clfs[\"CatBoostc\"] = CatBoostClassifier(\n",
    "            iterations=800, depth=8, learning_rate=0.06,\n",
    "            loss_function=\"MultiClass\", random_seed=0, verbose=False\n",
    "        )\n",
    "\n",
    "    preds = []\n",
    "    for name, clf in clfs.items():\n",
    "        clf.fit(Xtr_c, ytr_c)\n",
    "        yp = clf.predict(Xte)\n",
    "        acc = accuracy_score(yte_cls, yp)\n",
    "        cls_rows.append({\"h\": h, \"model\": name, \"accuracy\": acc})\n",
    "        preds.append(yp)\n",
    "\n",
    "    pred_arr = np.column_stack(preds)\n",
    "    vote = []\n",
    "    for i in range(pred_arr.shape[0]):\n",
    "        vals, cnts = np.unique(pred_arr[i], return_counts=True)\n",
    "        vote.append(vals[np.argmax(cnts)])\n",
    "    vote = np.array(vote)\n",
    "    cls_rows.append({\"h\": h, \"model\": \"VoteEnsemble\", \"accuracy\": accuracy_score(yte_cls, vote)})\n",
    "\n",
    "    cm = confusion_matrix(yte_cls, vote, labels=[0,1,2])\n",
    "    fig = plt.figure(figsize=(4,3))\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(f\"CO class +{h}h â€” Vote\")\n",
    "    plt.xticks([0,1,2], [\"Low\",\"Mid\",\"High\"])\n",
    "    plt.yticks([0,1,2], [\"Low\",\"Mid\",\"High\"])\n",
    "    for (ii, jj), v in np.ndenumerate(cm):\n",
    "        plt.text(jj, ii, str(v), ha='center', va='center')\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(FIG_DIR, f\"CO_cls_h{h}_cm.png\"), dpi=160)\n",
    "    plt.close(fig)\n",
    "\n",
    "cls_df = pd.DataFrame(cls_rows)\n",
    "cls_df.to_csv(os.path.join(OUTPUT_DIR, \"results_classification.csv\"), index=False)\n",
    "\n",
    "print(\"Artifacts:\")\n",
    "print(\" - outputs/results_regression.csv\")\n",
    "print(\" - outputs/results_regression_baseline.csv\")\n",
    "print(\" - outputs/results_classification.csv\")\n",
    "print(\" - outputs/figures/*\")\n",
    "print(f\"Robustness (CO +1h): base RMSE={rmse_base:.4f}, robust RMSE={rmse_rob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4668e93b-5512-4f84-8d4d-de7d92b41120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
